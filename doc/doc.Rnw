\documentclass[12pt, a4paper, oneside]{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{enumerate}

\newcommand\ci{\perp\!\!\!\perp}

% \usepackage{pdflscape}

% font size could be 10pt (default), 11pt or 12 pt
% paper size coulde be letterpaper (default), legalpaper, executivepaper,
% a4paper, a5paper or b5paper
% side coulde be oneside (default) or twoside 
% columns coulde be onecolumn (default) or twocolumn
% graphics coulde be final (default) or draft 
%
% titlepage coulde be notitlepage (default) or titlepage which 
% makes an extra page for title 
% 
% paper alignment coulde be portrait (default) or landscape 
%
% equations coulde be 
%   default number of the equation on the rigth and equation centered 
%   leqno number on the left and equation centered 
%   fleqn number on the rigth and  equation on the left side
%   
\title{MDM Assignment 2}
\author{Marco Vassena  \\
    4110161 \\
    \and 
    Philipp Hausmann \\
    4003373 \\
    }

\date{\today} 
\begin{document}

<<setup, echo=FALSE, cache=FALSE,include=FALSE>>=
source('../src/analysis.r', chdir=TRUE)
library(knitr)
library(xtable)
library(igraph)
# For answer (e) during lots of iterations, 
# loglin doesn't converge and prints warning messages (Warning: algorithm did not converge)
options(warn=-1)  
@

\maketitle

\tableofcontents


\section{Problem description}
The goal of this assignment is to implement and evaluate a tree classification algorithm.
In addition to that, we will also discuss two different implementation styles
and their performance implications.

\section{Analysis}


\begin{enumerate}[(a)]

%A
\item
The graph contains $10$ nodes, and there may or may not be an edge
between every node combination. The number of different graphical models
is therefore equal to $2^{10 \choose 2}$, which gives a value of $3.518437e+13$.

% R code: 2^choose(10,2)

%B
\item
% http://www.cs.uu.nl/docs/vakken/mdm/Slides/dm-graphmod-1.pdf, slide 7
The number of cells of the counts table can be computed by multiplying the number
of categories for each attribute together. This gives the expression
$9 * 2^3 * 3 * 6 * 4 * 3 * 5 * 2$, which is equal to $155520$.

The number of parameters is the number of parameters minus 1, as all probablities
have to sum up to $1$. Therefore there are $155519$ parameters.

% length(table(d)): 155520

%C
\item
The nodes have been numbered according to table \ref{tbl:ans-c-nodes}. The cliques of the
resulting model can be seen in table \ref{tbl:ans-c-cliq}. Figure \ref{grp:ans-c-graph} shows
the independence graph of the model.

\begin{table}
  \begin{center}
    \begin{tabular}{ | c | c | }
      \hline
      N & variable \\ \hline
      1 & cat1 \\ \hline
      2 & death \\ \hline
      3 & swang1 \\  \hline
      4 & gender \\ \hline
      5 & race \\ \hline
      6 & ninsclas \\ \hline
      7 & income \\ \hline
      8 & ca \\ \hline
      9 & age \\ \hline
      10 & meanbp1 \\ \hline
    \end{tabular}
    \caption{Numbering used to identify the variables of the model}
    \label{tbl:ans-c-nodes}
  \end{center}
\end{table}

<<answer-c,cache=TRUE,echo=FALSE>>=
rhc.graphmod.c <- answer.c()
@

% TODO maybe better table?
<<table-c,results='asis',cache=FALSE,echo=FALSE>>=
tbl <- xtable(clique.table(rhc.graphmod.c), 
              caption = "The cliques of the resulting model for question (c)",
              align = "cc",
              label = "tbl:ans-c-cliq")
print(tbl)
@

\begin{figure}
<<graph-c,fig.keep='high',cache=FALSE,echo=FALSE>>=
g.c <- from.cliques(rhc.graphmod.c$cliques)
plot.igraph(g.c, layout = layout.circle(g.c))
@
\caption{The independence graph for question (c)}
\label{grp:ans-c-graph}
\end{figure}


%D
\item 
The Pairwaise Markov property states that any two non-adjacent variables are conditionally
independent given all the others.
Since variable node 4 (variable \emph{gender}) and node 7 (variable \emph{income}) 
are not adjacent (the edge $(4,7)$ is not present in the graph), we can conclude that:
\[ X_4 \ci X_7 | (X_1, X_2, X_3, X_5, X_6, X_7, X_8, X_9, X_{10})\] 

Using the Global Markov property we can derive a stronger statement.
Node 4 is separated from node 7 by node 6, because any path from 4 to 7 passes
through node 6, therefore \emph{gender} is independent from \emph{income} given
\emph{ninsclas} (6).

\[ X_4 \ci X_7 | X_6 \]

By the Local Markov property a variable, given its closed neighbourhood, its independent
from all remaining vertices. Therefore, in order to estimate the 
variable \emph{death} (2) (which determines whether someone survives), it is sufficient to
consider its adjacents variables: \emph{ca} (8), \emph{age} (9), \emph{meanbp1} (10).

%E
\item 
The model found starting from the saturated model (complete graph) is slightly worse 
than the model found starting from the independence model (empty graph).
% TODO I actually expected a bigger difference, check!
The BIC score of the model is 15851, whereas the BIC score of the model found in (c) is
15842. 
The model found in (e) contains more cliques than the model found (c), 
namely: $\{2, 7\}, \{3, 4\}, \{3, 9\}, \{5, 7\}, \{5, 9\}$.
However model (c) contains the following cliques, which are not included in (e):
$\{1,9\},\{3,6\},\{5,6\}$

Because of the additional nodes departing from 4 and 7 we can derive weaker independence statements
about \emph{income} and \emph{gender}, namely:
\[ X_4 \ci X_7 | (X_6, X_3) \]
\[ X_4 \ci X_7 | (X_2, X_5, X_6)\]

Also for predicting variable $X_2$ \emph{death} we need to consider a larger set of variables,
because its neighborhood includes also \emph{income} (7), in addition to \emph{ca} (8), 
\emph{age} (9), \emph{meanbp1} (10) as before.

We would like to point out that during the analysis, the IPF algorithm used in the hill-climbing
search, often failed to converge, producing therefore models of lower quality.

<<answer-e,cache=TRUE,echo=FALSE>>=
rhc.graphmod.e <- answer.e()
@

% TODO maybe better table?
<<table-e,results='asis',cache=FALSE,echo=FALSE>>=
tbl <- xtable(clique.table(rhc.graphmod.e), 
              caption = "The cliques of the resulting model for question (e)",
              align = "cc")
print(tbl)
@

\begin{figure}
<<graph-e,fig.keep='high',cache=FALSE,echo=FALSE>>=
g.e <- from.cliques(rhc.graphmod.e$cliques)
plot.igraph(g.e, layout = layout.circle(g.e))
@
\caption{The independence graph for question (e)}
\end{figure}

%F
\item
<<answer-f,cache=TRUE,echo=FALSE>>=
rhc.graphmod.f <- answer.f()
@
<<table-f-compl,results='asis',cache=FALSE,echo=FALSE>>=
tbl <- xtable(clique.table(rhc.graphmod.f$complete),
            caption = "The cliques of the resulting model for question (f), starting from the complete graph.",
            align = "cc",
            label = "tbl:ans-f-compl")
print(tbl)
@
<<table-f-empty,results='asis',cache=FALSE,echo=FALSE>>=
tbl <- xtable(clique.table(rhc.graphmod.f$empty),
            caption = "The cliques of the resulting model for question (f), starting from the empty graph.",
            align = "cc",
            label = "tbl:ans-f-empty")
print(tbl)
@
<<answer-f-scores,cache=FALSE,echo=FALSE,include=FALSE>>=
print(rhc.graphmod.f$complete$score)
@
The scores of the models starting from the complete and empty graphs are equal and are $14278$.
In addition, both models have the same cliques as can be seen in table \ref{tbl:ans-f-compl} and \ref{tbl:ans-f-empty},
and therefore also the same graph.
This indicates that the same local optima is found from both starting points.


\item
The models computed using BIC score are simpler than those found using AIC.
The former have mostly small cliques (2 verteces) whereas the latter tend to have
bigger cliques (3 verteces).
The reason lies in the difference between BIC and AIC score, namely the weight
used for the $dim(M)$ component.
This factor represents the number of non-zero u-terms and determines the complexity of the model.
For a model $M$, a small $dim(M)$ means that many u-terms are set to zero, resulting in
a independence graph will smaller cliques.
For AIC the wheight used is the constant 2, whereas for BIC it is $ln(N)$ where $N$ is the number of 
observations in the data set. The data set used in this assignment contains 
5735 rows, thus $log(5735) \approx 8.654343 > 2$.
Using the BIC score the hill-climbing algorithm compensates the worse scores, due to
this greater weight producing simpler models, i.e. models with fewer non-zero u-terms
(smaller $dim(M)$, which result in smaller cliques.

%H
\item

<<answer-h,cache=TRUE,echo=FALSE>>=
rhc.h <- search.params()
@
<<answer-h-tbl,cache=FALSE,results='asis',echo=FALSE>>=
etab <- eval_to_df(rhc.h)
#ctab <- with(etab, tapply(error, list(nmin, minLeaf), sum))
#digits <- c(0, rep(3, dim(ctab)[2]))
tab <- xtable(etab, caption = "Result of training models with different parameters for question (h).", label = "tbl:ans-h")
print(tab)
@
We have searched for better models using the \texttt{restart} approach with different parameters.
For each parameter combination we performed 20 restarts, and both backward and forward search
was always enabled. The complete results can be seen in table \ref{tbl:ans-h}.

For the AIC scoring function, the restarting approach yields a minimally better result with a
AIC score of $14263$ for values of \texttt{prob} of $\{25\%, 50\%, 75\%\}$. The difference in AIC score
between these three values of \texttt{prob} is minimal and may well be just random noise.

For the BIC scoring function, the found models for \texttt{prob} values of
$\{25\%, 50\%\}$ have a score of $15783$. This is a minor improvement over the earlier models,
which had a best score of $15841$. 


\end{enumerate}

\begin{thebibliography}{1}

\bibitem{SPAM}
  Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt.
  \emph{SPAM E-mail Database}.
  Hewlett-Packard Labs, 1501 Page Mill Rd., 
  Palo Alto, CA 94304,
  June-July 1999.


\end{thebibliography}

\end{document}
